{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gpflow\n",
    "from gpflow.param import Param, Parameterized, AutoFlow\n",
    "from gpflow._settings import settings\n",
    "import tensorflow as tf\n",
    "float_type = settings.dtypes.float_type\n",
    "int_type = settings.dtypes.int_type\n",
    "np_float_type = np.float32 if float_type is tf.float32 else np.float64\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\", font_scale= 1.4)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.concat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendRBF1D(gpflow.kernels.Kern):\n",
    "    \"\"\"\n",
    "    Kernel for monotonicity models\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        gpflow.kernels.Kern.__init__(self, input_dim = 1, active_dims= [0])\n",
    "        self.lengthscale = gpflow.param.Param(1.0, transform=gpflow.transforms.positive)\n",
    "        self.signal_variance = gpflow.param.Param(1.0, transform=gpflow.transforms.positive)\n",
    "    \n",
    "    def Kn(self, X1, X2):\n",
    "        cov = self.signal_variance * tf.exp(-1./(2 * tf.square(self.lengthscale)) * tf.square(X1 - tf.transpose(X2)))\n",
    "        return cov\n",
    "    \n",
    "    def Kd(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Covariance between gaussian process at X2 and its derivative at X1\n",
    "        \"\"\"\n",
    "        cov = (self.signal_variance * \n",
    "               tf.exp(-1./(2 * tf.square(self.lengthscale)) * tf.square(X1 - tf.transpose(X2))) *\n",
    "              (-1./tf.square(self.lengthscale) * (X1 - tf.transpose(X2))))\n",
    "        return cov\n",
    "    \n",
    "    def Kdd(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Covariance between derivatives of gaussian process at X1 and X2\n",
    "        \"\"\"\n",
    "        cov = (self.signal_variance * \n",
    "               tf.exp(-1./(2 * tf.square(self.lengthscale)) * tf.square(X1 - tf.transpose(X2)))* \n",
    "               1./tf.square(self.lengthscale) *\n",
    "              (1. - 1./tf.square(self.lengthscale) * tf.square(X1 - tf.transpose(X2))))\n",
    "        return cov\n",
    "    \n",
    "    def K(self, X, X_prime):\n",
    "        \"\"\"\n",
    "        Covariance matrix for joint Normal distribution over GP and its derivative\n",
    "        \"\"\"\n",
    "        K_f_f = self.Kn(X, X)\n",
    "        K_f_fprime = self.Kd(X, X_prime)\n",
    "        K_fprime_f = tf.transpose(K_f_fprime)\n",
    "        K_fprime_fprime = self.Kdd(X_prime, X_prime)\n",
    "        Knew1 = tf.concat([K_f_f, K_f_fprime], 1)\n",
    "        Knew2 = tf.concat([K_fprime_f, K_fprime_fprime], 1)\n",
    "        K_joint = tf.concat([Knew1, Knew2], 0)\n",
    "        return K_joint \n",
    "        \n",
    "    @AutoFlow((float_type, [None, None]), (float_type, [None, None]))\n",
    "    def compute_Kn(self, X, Z):\n",
    "        return self.Kn(X, Z)\n",
    "    \n",
    "    @AutoFlow((float_type, [None, None]), (float_type, [None, None]))\n",
    "    def compute_Kd(self, X, Z):\n",
    "        return self.Kd(X, Z)\n",
    "    \n",
    "    @AutoFlow((float_type, [None, None]), (float_type, [None, None]))\n",
    "    def compute_Kdd(self, X, Z):\n",
    "        return self.Kdd(X, Z)\n",
    "    \n",
    "    @AutoFlow((float_type, [None, None]), (float_type, [None, None]))\n",
    "    def compute_K(self, X, X_prime):\n",
    "        return self.K(X, X_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1,2,3])[:,None]\n",
    "X_prime = np.array([4,5])[:,None]\n",
    "K = ExtendRBF1D()\n",
    "aa = K.compute_K(X, X_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gpflow.likelihoods import Likelihood\n",
    "from gpflow.likelihoods import probit\n",
    "from gpflow import densities\n",
    "\n",
    "class MonotoneLikelihood(Likelihood):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Likelihood for Gaussian Process with monotonicity constraints\n",
    "        \"\"\"\n",
    "        Likelihood.__init__(self)\n",
    "        self.nu = 1e-6\n",
    "        self.signal_variance = Param(1.0, transforms.positive)\n",
    "    \n",
    "    def logp_ygf(self, F, Y):\n",
    "        return tf.reduce_sum(densities.gaussian(F, Y, self.signal_variance))\n",
    "    \n",
    "    def logp_m(self, F_prime, invlink = probit):\n",
    "        Y = tf.ones(tf.shape(F_prime).shape.as_list(), tf.int32) \n",
    "        return tf.reduce_sum(densities.bernoulli(self.invlink(1./self.nu*F_prime), Y))\n",
    "    \n",
    "    def logp(F, Y, F_prime):\n",
    "        log_like_ygp = self.logp_ygf(F, Y)\n",
    "        log_like_m = self.logpm(F_prime, invlink = probit)\n",
    "        log_like = log_like_ygp + log_like_m\n",
    "        return log_like    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpflow.mean_functions import Zero\n",
    "from gpflow.param import Parameterized, AutoFlow, DataHolder\n",
    "\n",
    "class MonotoneGP(gpflow.model.Model):\n",
    "    def __init__(self, X_concat, Y, name = 'monotonic_model'):\n",
    "        \"\"\"\n",
    "        X_concat is a data vector, size (N + M) x 1\n",
    "        X_concat = (X, X_der_loc)\n",
    "        Y is a data matrix, size N x 1 \n",
    "    \n",
    "        This is a vanilla implementation of a GP with monotonicity contraints.\n",
    "        \n",
    "        Refer:\n",
    "        https://bayesopt.github.io/papers/2017/9.pdf\n",
    "        \n",
    "        \"\"\"\n",
    "        if not X_concat.shape[1] == 1:\n",
    "            raise ValueError('Currently, GP with monotonicity is only supported for 1D')\n",
    "        \n",
    "        # Initialize the model\n",
    "        gpflow.Model.__init__(self, name)\n",
    "        # Zero mean function for now\n",
    "        self.mean_function = Zero()\n",
    "        \n",
    "        # Initialize data\n",
    "        if isinstance(X_concat, np.ndarray):\n",
    "            #: X is a data matrix; each row represents one instance\n",
    "            X_concat = DataHolder(X_concat)\n",
    "        if isinstance(Y, np.ndarray):\n",
    "            #: Y is a data matrix, rows correspond to the rows in X, columns are treated independently\n",
    "            Y = DataHolder(Y)\n",
    "        \n",
    "        # Define kernel\n",
    "        self.kern = ExtendRBF1D()\n",
    "        \n",
    "        # Define likelihood \n",
    "        self.likelihood = MonotoneLikelihood()\n",
    "        self.likelihood._check_targets(Y.value)\n",
    "        \n",
    "        # Initialize\n",
    "        self.Y = Y\n",
    "        self.X_concat = X_concat\n",
    "        \n",
    "        self._session = None\n",
    "    \n",
    "    def build_predict(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @AutoFlow((float_type, [None, None]))\n",
    "    def predict_f(self, Xnew):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of the latent function(s)\n",
    "        at the points `Xnew`.\n",
    "        \"\"\"\n",
    "        return self.build_predict(Xnew)\n",
    "\n",
    "    @AutoFlow((float_type, [None, None]))\n",
    "    def predict_f_full_cov(self, Xnew):\n",
    "        \"\"\"\n",
    "        Compute the mean and covariance matrix of the latent function(s) at the\n",
    "        points Xnew.\n",
    "        \"\"\"\n",
    "        return self.build_predict(Xnew, full_cov=True)\n",
    "\n",
    "    @AutoFlow((float_type, [None, None]), (tf.int32, []))\n",
    "    def predict_f_samples(self, Xnew, num_samples):\n",
    "        \"\"\"\n",
    "        Produce samples from the posterior latent function(s) at the points\n",
    "        Xnew.\n",
    "        \"\"\"\n",
    "        mu, var = self.build_predict(Xnew, full_cov=True)\n",
    "        jitter = tf.eye(tf.shape(mu)[0], dtype=float_type) * settings.numerics.jitter_level\n",
    "        samples = []\n",
    "        for i in range(self.num_latent):\n",
    "            L = tf.cholesky(var[:, :, i] + jitter)\n",
    "            shape = tf.stack([tf.shape(L)[0], num_samples])\n",
    "            V = tf.random_normal(shape, dtype=settings.dtypes.float_type)\n",
    "            samples.append(mu[:, i:i + 1] + tf.matmul(L, V))\n",
    "        return tf.transpose(tf.stack(samples))\n",
    "\n",
    "    @AutoFlow((float_type, [None, None]))\n",
    "    def predict_y(self, Xnew):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of held-out data at the points Xnew\n",
    "        \"\"\"\n",
    "        pred_f_mean, pred_f_var = self.build_predict(Xnew)\n",
    "        return self.likelihood.predict_mean_and_var(pred_f_mean, pred_f_var)\n",
    "\n",
    "    @AutoFlow((float_type, [None, None]), (float_type, [None, None]))\n",
    "    def predict_density(self, Xnew, Ynew):\n",
    "        \"\"\"\n",
    "        Compute the (log) density of the data Ynew at the points Xnew\n",
    "\n",
    "        Note that this computes the log density of the data individually,\n",
    "        ignoring correlations between them. The result is a matrix the same\n",
    "        shape as Ynew containing the log densities.\n",
    "        \"\"\"\n",
    "        pred_f_mean, pred_f_var = self.build_predict(Xnew)\n",
    "        return self.likelihood.predict_density(pred_f_mean, pred_f_var, Ynew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gpflow.priors import Gaussian\n",
    "class MonotoneGPMC(MonotoneGP):\n",
    "    def __init__(self, X, Y, X_prime, num_latent = None):\n",
    "        \"\"\"\n",
    "        X is a data vector, size N x 1\n",
    "        X_prime is a vector, size M x 1\n",
    "        Y is a data matrix, size N x 1 \n",
    "    \n",
    "        This is a vanilla implementation of a GP with monotonicity contraints and HMC sampling\n",
    "        Refer:\n",
    "        https://bayesopt.github.io/papers/2017/9.pdf\n",
    "        \"\"\"\n",
    "        X_concat = np.vstack([X, X_prime])\n",
    "        X_concat = DataHolder(X_concat, on_shape_change='recompile')\n",
    "        Y = DataHolder(Y, on_shape_change='recompile')\n",
    "        MonotoneGP.__init__(self, X_concat, Y)\n",
    "        self.num_data = X_concat.shape[0]\n",
    "        self.num_x_points = X.shape[0]\n",
    "        self.num_der_points = X_prime.shape[0]\n",
    "        self.num_latent = num_latent or Y.shape[1]\n",
    "        self.V = Param(np.zeros((self.num_data, self.num_latent)))\n",
    "        self.V.prior = Gaussian(0., 1.)\n",
    "    def compile(self, session = None, graph = None, optimizer = None):\n",
    "        \"\"\"\n",
    "        Before calling the standard compile function, check to see if the size\n",
    "        of the data has changed and add parameters appropriately.\n",
    "\n",
    "        This is necessary because the shape of the parameters depends on the\n",
    "        shape of the data.\n",
    "        \"\"\"\n",
    "        if not self.num_data == self.X_concat.shape[0]:\n",
    "            self.num_data = self.X_concat.shape[0]\n",
    "            self.V = Param(np.zeros((self.num_data, self.num_latent)))\n",
    "            self.V.prior = Gaussian(0., 1.)\n",
    "        \n",
    "        return super(MonotoneGPMC, self).compile(session = session,\n",
    "                                                 graph = graph,\n",
    "                                                 optimizer = optimizer)\n",
    "    \n",
    "    def build_likelihood(self):\n",
    "        K = self.kern.K(self.X)\n",
    "        L = tf.cholesky(K + tf.eye(tf.shape(self.X)[0], dtype=float_type)*settings.numerics.jitter_level)\n",
    "        F_concat = tf.matmul(L, self.V) + self.mean_function(self.X)\n",
    "        F, F_prime = tf.split(F_concat, [self.num_x_points, self.num_der_points])\n",
    "        log_like = self.likelihood.logp(F, self.Y, F_prime)\n",
    "        return log_like\n",
    "    \n",
    "    def build_predict(self, Xnew, full_cov=False):\n",
    "        \"\"\"\n",
    "        Xnew is a data matrix, point at which we want to predict\n",
    "\n",
    "        This method computes\n",
    "\n",
    "            p(F* | (F=LV) )\n",
    "\n",
    "        where F* are points on the GP at Xnew, F=LV are points on the GP at X.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        raise ValueError(\"Under construction\")\n",
    "        \n",
    "        mu, var = conditional(Xnew, self.X, self.kern, self.V,\n",
    "                              full_cov=full_cov,\n",
    "                              q_sqrt=None, whiten=True)\n",
    "        return mu + self.mean_function(Xnew), var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 10)\n",
    "x_der = np.linspace(0,1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.05263158, 0.10526316, 0.15789474, 0.21052632,\n",
       "       0.26315789, 0.31578947, 0.36842105, 0.42105263, 0.47368421,\n",
       "       0.52631579, 0.57894737, 0.63157895, 0.68421053, 0.73684211,\n",
       "       0.78947368, 0.84210526, 0.89473684, 0.94736842, 1.        ])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@NameScoped(\"conditional\")\n",
    "def conditional(Xnew, X, kern, f, full_cov=False, q_sqrt=None, whiten=False):\n",
    "    \"\"\"\n",
    "    Given F_concat, representing the GP at the points X,\n",
    "    produce the mean and (co-)variance of the GP at the points Xnew.\n",
    "\n",
    "    Additionally, there may be Gaussian uncertainty about F as represented by\n",
    "    q_sqrt. In this case `f` represents the mean of the distribution and\n",
    "    q_sqrt the square-root of the covariance.\n",
    "\n",
    "    Additionally, the GP may have been centered (whitened) so that\n",
    "        p(v) = N( 0, I)\n",
    "        f = L v\n",
    "    thus\n",
    "        p(f) = N(0, LL^T) = N(0, K).\n",
    "    In this case 'f' represents the values taken by v.\n",
    "\n",
    "    The method can either return the diagonals of the covariance matrix for\n",
    "    each output of the full covariance matrix (full_cov).\n",
    "\n",
    "    We assume K independent GPs, represented by the columns of f (and the\n",
    "    last dimension of q_sqrt).\n",
    "\n",
    "     - Xnew is a data matrix, size N x D\n",
    "     - X are data points, size M x D\n",
    "     - kern is a GPflow kernel\n",
    "     - f is a data matrix, M x K, representing the function values at X, for K functions.\n",
    "     - q_sqrt (optional) is a matrix of standard-deviations or Cholesky\n",
    "       matrices, size M x K or M x M x K\n",
    "     - whiten (optional) is a boolean: whether to whiten the representation\n",
    "       as described above.\n",
    "\n",
    "    These functions are now considered deprecated, subsumed into this one:\n",
    "        gp_predict\n",
    "        gaussian_gp_predict\n",
    "        gp_predict_whitened\n",
    "        gaussian_gp_predict_whitened\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # compute kernel stuff\n",
    "    num_data = tf.shape(X)[0]  # M\n",
    "    num_func = tf.shape(f)[1]  # K\n",
    "    Kmn = kern.K(X, Xnew)\n",
    "    Kmm = kern.K(X) + tf.eye(num_data, dtype=float_type) * settings.numerics.jitter_level\n",
    "    Lm = tf.cholesky(Kmm)\n",
    "\n",
    "    # Compute the projection matrix A\n",
    "    A = tf.matrix_triangular_solve(Lm, Kmn, lower=True)\n",
    "\n",
    "    # compute the covariance due to the conditioning\n",
    "    if full_cov:\n",
    "        fvar = kern.K(Xnew) - tf.matmul(A, A, transpose_a=True)\n",
    "        shape = tf.stack([num_func, 1, 1])\n",
    "    else:\n",
    "        fvar = kern.Kdiag(Xnew) - tf.reduce_sum(tf.square(A), 0)\n",
    "        shape = tf.stack([num_func, 1])\n",
    "    fvar = tf.tile(tf.expand_dims(fvar, 0), shape)  # K x N x N or K x N\n",
    "\n",
    "    # another backsubstitution in the unwhitened case\n",
    "    if not whiten:\n",
    "        A = tf.matrix_triangular_solve(tf.transpose(Lm), A, lower=False)\n",
    "\n",
    "    # construct the conditional mean\n",
    "    fmean = tf.matmul(A, f, transpose_a=True)\n",
    "\n",
    "    if q_sqrt is not None:\n",
    "        if q_sqrt.get_shape().ndims == 2:\n",
    "            LTA = A * tf.expand_dims(tf.transpose(q_sqrt), 2)  # K x M x N\n",
    "        elif q_sqrt.get_shape().ndims == 3:\n",
    "            L = tf.matrix_band_part(tf.transpose(q_sqrt, (2, 0, 1)), -1, 0)  # K x M x M\n",
    "            A_tiled = tf.tile(tf.expand_dims(A, 0), tf.stack([num_func, 1, 1]))\n",
    "            LTA = tf.matmul(L, A_tiled, transpose_a=True)  # K x M x N\n",
    "        else:  # pragma: no cover\n",
    "            raise ValueError(\"Bad dimension for q_sqrt: %s\" %\n",
    "                             str(q_sqrt.get_shape().ndims))\n",
    "        if full_cov:\n",
    "            fvar = fvar + tf.matmul(LTA, LTA, transpose_a=True)  # K x N x N\n",
    "        else:\n",
    "            fvar = fvar + tf.reduce_sum(tf.square(LTA), 1)  # K x N\n",
    "    fvar = tf.transpose(fvar)  # N x K or N x N x K\n",
    "\n",
    "    return fmean, fvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])\n",
    "a = tf.shape(t)  # [2, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method TensorShape.as_list of TensorShape([Dimension(3)])>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.eye(2)\n",
    "c = np.eye(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = DataHolder(a)\n",
    "d = DataHolder(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gpflow.param.DataHolder at 0x1a232b21d0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Tensors in list passed to 'values' of 'ConcatV2' Op have types [<NOT CONVERTIBLE TO TENSOR>, <NOT CONVERTIBLE TO TENSOR>] that don't all match.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-8f37b17f6274>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/nimishawalgaonkar/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1179\u001b[0m               tensor_shape.scalar())\n\u001b[1;32m   1180\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nimishawalgaonkar/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.pyc\u001b[0m in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[0m_attr_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 949\u001b[0;31m         \"ConcatV2\", values=values, axis=axis, name=name)\n\u001b[0m\u001b[1;32m    950\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nimishawalgaonkar/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    481\u001b[0m                                 (prefix, dtype.name))\n\u001b[1;32m    482\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s that don't all match.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m               \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s that are invalid.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Tensors in list passed to 'values' of 'ConcatV2' Op have types [<NOT CONVERTIBLE TO TENSOR>, <NOT CONVERTIBLE TO TENSOR>] that don't all match."
     ]
    }
   ],
   "source": [
    "r = tf.concat(values = [g, f], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.concat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_pos_def(x):\n",
    "    return np.all(np.linalg.eigvals(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_pos_def(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
